Unison Loading and Updating				-*-outline-*-

This document describes the Unison loading and release processes. It is
intended for developers and maintainers of a Unison database.  In this
document, "loading" means both "loading from scratch" and "updating an
existing database" -- the processes are essentially identical.


* The Big Picture
The csb-dev instance is the only database which is ever modified. Period.
Other versions of the database are read-only (from my point of view).

csb-dev contents (sequences, results, sets, etc) are loaded/updated as
described extensively below. Once a round of loading is completed, Unison
is "staged", typically in conjunction with the tools and web
components. The staged database, tools, and web interface are tested;
fixes are made to the development code and the components are restaged as
necessary.  Finally, the database is then renamed to push it into
production; the code and web pages are committed to CVS, tagged, and tar'd
up. A backup is made of the csb (production) database. That backup is then
filtered by a perl script to build a public version database as a
psql-dump.  That dump is then restored to verify its operation.


* General loading 
Unison loading and updating is mostly achieved through makefiles using
scripts in unison/sbin. There's is an ongoing effort to unify the
operation of these, but some tweaking and debugging may still be required.

Loading occurs in these phases:
- Load auxiliary data
- Load sequences
- Load models (HMMs, threading backbones, etc)
- Infer tax_ids for loaded sequences.
- Build sequence sets based on simple sequence data (init met, length)
  or associated data (origin, tax_id). This includes building sets of
  sequences for which results should be precomputed.
- Compute and load results.
- Manually drop obsolete params and/or update pftype(preferred_params_id)
- Compute statistics.
- Build sequence sets based on computed results.

Unison is designed for repeated loading by the above processes.  Unison
provides sufficient bookkeeping that only the new or updated data are
loaded.

Data are always loaded by the 'loader' user. The exact permissions on a
table for insert, update, or delete depend on the needs for that data
source.



* Auxiliary data loading
Auxiliary data refers to third party loaded into distinct schemas.  This
includes scop, swiss-prot taxonomy, go/gong, pdb, and others.  The intent
is to incorporate these with no (or very minor) changes.  These are
intentially distinct from the Unison schema and not dependent on it in any
way.

These schemas have same-named subdirectories in unison/aux/, and typically
Makefiles within which load or update the schemas.

Current auxiliary schemas:
** go (and GOng)
** pdb
** scop
** tax
** sst (aka GenenGenes; Genentech specific)

NB: homologene and MINT were errantly loaded into unison itself and
will be moved to their own schemas.


* Sequence loading
Sequences may be loaded from any source recognized by bioperl's SeqIO
module. Currently, fasta and Refseq-formatted files are loaded routinely.
See loading/pseq/Makefile for examples.

Sequences are loaded with aliases and annotations into origins. An alias
must be unique within the origin; if it is not, the previous sequence is
deprecated and the new sequence assumes the alias.

A typical data loading invocation is:
$ PGPASSWORD=thepass make Refseq.load


* Model loading
Models are any entity to which a sequence may be aligned. Currently,
models are other sequences, HMMs, PSSMs, structure templates, or regular
expressions.

** Prospect2 templates
** HMMs (Pfam, in-house)
** PSSMs (Proceryon, CDD)
** genome assemblies
** regexps


* Data derived in Unison (e.g., by plpgsql code):
** infer_tax_id
select update_tax_ids();

** best_annotation_mv

** update sets
select update_psets_intrinsic();




* Precomputed results
Results are typically precomputed for the runA, runB, or runC set,
depending on the computational cost.

A typical data loading invocation is:
$ export PGPASSWORD=thepass
$ make runA.ids runA-todo.ids
$ make runA-todo.load (or) make qsub/runA-todo.load

You may also break large runs into pieces:
$ make runA.ids runA-todo.ids
$ make runA-todo-l100
  breaks runA-todo.ids into 100-id files in a new runA-todo-l100/ directory
$ make runA-todo-l100-load (or) runA-todo-l100-qload
  runs (or submits to qsub) each of the 100-id files



** BLAST (against reliable sequences and PDB sequences)
** run-blat
** run-pahmm
** run-paprospect2
** run-papssm
** run-pfregexp
** run-protcomp; 
** run-tmdetect (in-house); 1500/min
** run-tmhmm
secondary structure
sigcleave
antigenic
bigpi
signalp


* Extrinsic sequence sets
secreted
tm (1,5,7)
intracellular
PM
patented
typeI
typeII


* Post-loading tasks

** generate schema docs

** best_annotation_mv

** update stats

** load new scop
scop2pmodel 

** release / stats

** move blast database files to common location

** Manually drop obsolete params and/or update pftype(preferred_params_id)



* database flow and backups

web:    8080              8040              80                  8000    
db:   csb-dev  -------->  stage  -------->  csb                unison
         |     dev2stage         stage2prd   |                   ^
         |backup (cron)                      |publicize    |restore
         v                                   v                   |
bu:   csb-dev-<ts>                        csb-<ts>  ------->  unison-<ts>
                                                  

bu = backup
ts = timestamp


